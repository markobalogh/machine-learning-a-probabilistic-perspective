\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry} % to modify margins
\usepackage[T1]{fontenc}
\usepackage{palatino}
\usepackage{amsthm}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=1pt,topsep=2pt}
\setlist[itemize]{itemsep=1pt,topsep=2pt}
\usepackage{hyperref}
\hypersetup{colorlinks=true,citecolor=blue,pdfpagemode=UseNone}
\usepackage{amsfonts,amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{authblk}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{color}

%Custom commands go here!
\newcommand\calA{\mathcal{A}}
\newcommand\calB{\mathcal{B}}
\newcommand\calF{\mathcal{F}}
\newcommand\calO{\mathcal{O}}
\newcommand\calP{\mathcal{P}}
\newcommand\calS{\mathcal{S}}
\newcommand\calX{\mathcal{X}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Theorem environments go here!
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{remark}{Remark}
\newtheorem*{fact}{Fact}
\newtheorem{example}{Example}

\title{Machine Learning: A Probabilistic Perspective}

\begin{document}
\maketitle

\section{Introduction}
    \subsection{Types of machine learning}
        \paragraph{Predictive or Supervised Learning}
            The goal is to learn a mapping from inputs to outputs given a labeled set of input-output pairs.
        \paragraph{Descriptive or Unsupervised Learning}
            The goal is to find interesting patterns in a given dataset.
    \subsection{Supervised Learning}
        \paragraph{Classification}
            Learning a mapping from inputs $x$ to outputs $y$ where $y\in \{1,..., C\}$, with $C$ being the number of classes.
            Classification can be formalized as \textbf{function approximation}.
            Assume $y=f(x)$ for some unknown function $f$. We estimate $f$ given a labeled training set, and make predictions from the estimated function, which we denote $\hat{f}(x)$.
            \subparagraph{Binary Classification}    
                Where the number of classes is 2.
            \subparagraph{Multiclass Classification}
                Where the number of classes is greater than 2.
            \subparagraph{Multi-label classification}
                Where each instance may belong to multiple classes.
            \subparagraph{Generalization}
                Making predictions on inputs not in the training set.
            \subparagraph{Probabilistic Predictions}
                Given a model that outputs a probability for each class, we can make our "best guess" for the class of the instance by returning the most probable class label; the mode of the distribution. This is called the \textbf{MAP Prediction} or \textbf{Maximum A Posteriori}, meaning that the class has the highest likelihood given the instance.
                \begin{equation}
                    \hat{y}=\hat{f}(x)=argmax_c\ p(y=c|x,D)
                \end{equation}
                where $D$ is the training dataset.
            \subparagraph{Real World Applications of Classification}
                \begin{itemize}
                    \item Document Classification
                    \item Spam filtering
                    \item Image Classification
                    \item Handwriting Recognition
                    \item Face detection and Recognition
                \end{itemize}
        \paragraph{Regression}
            Like classification except that the response variable you are trying to predict is continuous.
    \subsection{Unsupervised Learning}
        The goal is to discover \emph{structure} in the data; this is sometimes called \emph{knowledge discovery}.
        Can be formalized as \textbf{density estimation}, meaning building models that return a probability density for each input: $p(x_i|\theta)$.
        \textbf{Supervised and unsupervised learning can be distinguished as conditional and unconditional density estimation, respectively.}
        \subsubsection{Clustering}
            The problem of separating data into groups. Let $K$ denote the number of clusters. The first goal is to estimate a probability distribution over the number of clusters, $p(K|D)$. The second goal is to estimate which cluster each point belongs to. The cluster each point belongs to would be known as a \emph{latent} or \emph{hidden} variable, because it isn't observed directly in the data. 
            \paragraph{Discovering Latent Factors}
                It's often useful to reduce the dimensionality of data to a lower dimensional subspace that caputres the "essence" of the data. This is called \textbf{dimensionality reduction}.

                The motivation is that the raw data may be high dimensional but there may be only a small number of degrees of variability, corresponding to latent factors.
                \subparagraph{Discovering Graph Structure}
                    Sometimes we measure a set of correlated variables, and we would like to discover which ones are most correlated with others. This can be represented by a graph $G$, in which nodes represent variables, and edges represent direct dependence between variables
            \paragraph{Matrix Completion} 
                Sometimes we have missing information, and inferring plausible values for missing values from existing values is called \textbf{imputation}. This is sometimes called \emph{matrix completion}.
                An example application is \emph{image inpainting}, in which holes in an image are filled in with realistic textures and shapes.
                
                Inferring values in an extremely large, extremely sparse matrix is sometimes called \emph{collaborative filtering}.

\end{document}